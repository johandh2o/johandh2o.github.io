{
  "hash": "7a084bd9232b19fc89aa27d0b2ac81d9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Recovering from selection bias with IPW methods\"\ndescription: \"On the consistency of IPW methods to recover causal effects from selection bias\"\ndate: \"2023-03-01\"\ncategories: [IPW, selection bias]\nfontsize: 11pt\nformat: \n  html:\n    fig-width: 7.5\n    fig-height: 4\n    fig-align: center\n    code-fold: true\n    toc: true\nbibliography: references.bib\nimage: logo.png\n---\n\n------------------------------------------------------------------------\n\n*Confounding bias* and *selection bias* are together the most prevalent hurdles to the validity and generalizability of causal inference results. In general, both arise from uncontrolled extraneous flows of statistical information between treatment and outcome in the analysis. Their precise characterization in the Structural Causal Models framework (SCM), and potential corrections, differ in nature:\n\n-   **Confounding bias** emerges from *open backdoor paths* between treatment and outcome in the directed acyclic graph (DAG) that represents the set of assumptions on the system's causal mechanisms. In experimental settings, (conditional) randomization of treatment assignment provides a practical solution to the problem. In observational studies, the *do*-calculus, developed by Judea Pearl [@pearl2009], constitute a complete and formal set of graphical rules to test the identifiability of the target causal parameters.\n\n-   **Selection bias** emerges from the preferential exclusion of units from the sample under analysis. It implies conditioning on a *collider* (or a descendant of a *virtual collider*), which opens backdoor paths between treatment and outcome. Besides, distributions learnt from the biased samples might not generalize to the whole population. Selection bias cannot be removed via randomization, which makes it a concern for experimental settings as well. There exist extensions of the *do*-calculus to deal with selection bias in some cases. *Inverse probability weighting* (IPW) methods are more frequently used in applied work.\n\n# Corrections for selection bias\n\n*Inverse probability weighting* (IPW) methods provide nonparametric statistical techniques to perform inference with biased data. In their basic presentation, they approximate the *interventional mean* of the outcome using a weighted average over the selected sample. Let:\n\n-   $(y_i)_{i=1}^N$ be a sample of outcomes for the selected (non-excluded) units\n\n-   $w_i$ be the relative inverse probability of selection, $w_i=p/p_i$, where $p$ is the population-level probability of selection, and $p_i$ is the probability of selection for unit $i$\n\n-   $v_i(a)$ be the inverse probability of assignment of treatment $a$ for unit $i$\n\nThe idea of IPW methods is motivated by the desired asymptotic result:\n\n$$\n\\lim_{N\\rightarrow\\infty} N^{-1}\\sum_{i=1}^N \\mathbb{I}[A_i=a] v_i(a)\\cdot w_i\\cdot y_i\\longrightarrow \\mathbb{E}[Y\\mid do(A=a)]\n$$\n\nThe derived finite-sample estimator is consistent if $v_i(a)\\cdot w_i$ is; this is, if the probability of selection and the probability of treatment assignment are jointly correctly specified.\n\nLet us analyse the results of an IPW-based approach on a simple simulation with a randomized treatment assignment and when:\n\n1.  the selection mechanism only hides the outcome. This is, only $Y$ is missing when $S=0$\n\n2.  the selection mechanism does not depend on a mediator\n\n## Simple simulation setting\n\nLet us consider the SCM given by the following DAG and set of structural equations:\n\n## The DAG:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  M ~ A,\n  S ~ A + L,\n  Y ~ A + M + L\n) %>% tidy_dagitty(layout = \"nicely\") %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=480}\n:::\n:::\n\n\n## Structural equations:\n\n$$\n\\begin{aligned}[c]\nL &\\sim\\text{Nor}(0,\\sigma^2_L) & &\\\\\nA &\\sim\\text{Ber}(q) & &\\\\\nM &= \\alpha_0 + \\alpha_1A + u_M & u_M &\\sim\\text{Nor}(0,\\sigma^2_M) \\\\\nS &= \\mathbb{I}[\\gamma_0 + \\gamma_1A + \\gamma_2M + \\gamma_3L + u_S > 0] & u_S &\\sim\\text{Nor}(0,\\sigma^2_S) \\\\\nY &= \\beta_0 + \\beta_1A + \\beta_2M + \\beta_3L + u_Y & u_Y &\\sim\\text{Nor}(0,\\sigma^2_Y) \n\\end{aligned}\n$$\n\n## Selection mechanism:\n\nWhen $S=1$ for a particular unit, we get to observe their whole data $(L,A,M,Y)$. When $S=1$, only the final outcome $Y$ is missing, so we get to observe $(L,A,M)$.\n\nLet us put this SCM as a data-generating process (DGP) in *R* code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Libraries needed\nlibrary(DescTools)\nlibrary(LaplacesDemon)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(modelsummary)\nlibrary(gt)\nlibrary(zeallot)\nlibrary(reshape2)\n\n# Data generating process\ndgp = function(param){\n  \n  #### Parameters\n  c(n,      # Number of samples\n    sdl,    # Stdr. dev. of selection predictor\n    p,      # Treatment assigment probability\n    a0, a1, # Parameters of A->M relation\n    sdm,    # Stdr. dev. of mediator noise\n    g0, g1, # Parameters of A->S relation\n    g2,     # Parameter of M->S relation\n    g3,     # Parameter of L->S relation\n    sds,    # Stdr. dev. of selection noise\n    b0, b1, # Parameters of A->Y relation\n    b2,     # Parameter of M->Y relation\n    b3,     # Parameter of L->Y relation\n    sdy # Stdr. dev. of selection noise\n    ) %<-% param     \n  \n  # Exogenous selection predictor\n  L = rnorm(n=n, mean=0, sd=sdl)\n  \n  # Treatment assigment\n  A = rbinom(n=n, size=1, prob=p)\n  \n  # Mediator\n  noise.M = rnorm(n=n, mean=0, sd=sdm)\n  M = a0 + a1*A + noise.M\n  \n  # Selection mechanism\n  noise.S = rnorm(n=n, mean=0, sd=sds)\n  S = ifelse(g0 + g1*A + g2*M + g3*L + noise.S > 0, 1, 0)\n  \n  # Outcome\n  noise.Y = rnorm(n=n, mean=0, sd=sdy)\n  Y = b0 + b1*A + b2*M + b3*L + noise.Y\n  \n  # true ATE\n  true.ATE = b1 + b2*a1\n  \n  # Data\n  dat = data.frame(L,A,M,S,Y)\n  \n  # Return\n  return(list(dat,true.ATE))\n}\n```\n:::\n\n\n------------------------------------------------------------------------\n\n# Simulation excercise\n\nThis case of $M$ not being a direct cause of $S$ is equivalent to setting $\\gamma_2=0$ in the structural equation of the selection mechanism $S$. Let us simulate some noisy data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set random seed\nset.seed(7)\n\n# Parameters values\nparam.1 = c(\n  n=1e3,             # Number of samples\n  sdl=1,             # Stdr. dev. of selection predictor\n  p=0.5,             # Treatment assigment probability\n  a0=0.1, a1=0.5,    # Parameters of A->M relation\n  sdm=0.4,           # Stdr. dev. of mediator noise\n  g0=-0.1, g1=0.2,   # Parameters of A->S relation\n  g2=0,              # Parameter of M->S relation ### M DOES NOT CAUSE S\n  g3=0.2,            # Parameter of L->S relation\n  sds=0.5,           # Stdr. dev. of selection noise\n  b0=-1.5, b1=0.5,   # Parameters of A->Y relation\n  b2=0.5,            # Parameter of M->Y relation\n  b3=1.8,            # Parameter of L->Y relation\n  sdy=2.7)           # Stdr. dev. of selection noise\n\n# Generate data\ndgp.1 = dgp(param.1)\n\n# Save the full data\ndat.1 = dgp.1[[1]]\n\n# Save the true ATE\nT.ATE = dgp.1[[2]]\n\n# Glimpse of the data\nhead(dat.1) %>% kbl() %>% kable_styling(full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> L </th>\n   <th style=\"text-align:right;\"> A </th>\n   <th style=\"text-align:right;\"> M </th>\n   <th style=\"text-align:right;\"> S </th>\n   <th style=\"text-align:right;\"> Y </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 2.2872472 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.2988695 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -0.0302749 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -1.1967717 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.5175524 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 3.7544911 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -0.6942925 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.0183553 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -3.2428682 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -0.4122930 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.3229087 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2.8405141 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -0.9706733 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.7833466 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -4.9430929 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -0.9472799 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -0.6778373 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -5.7906748 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWe can compute the true ATE underlying this SCM:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# True ATE\ndata.frame('True ATE'=T.ATE) %>% kbl() %>% kable_styling(full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> True.ATE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.75 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Results under no exclusion (complete data)\n\nFor a moment let us pretend we observe all variables for everyone. Let us examine the most interesting models that we can fit with the simulated data, to check their ability to recover population-level parameters under noisy samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model for M, with complete data\nmod.M = lm(M ~ A, data=dat.1)\n\n# Model for S, with complete data\nmod.S = glm(S ~ A + L, family=binomial('probit'), data=dat.1)\n\n# Model for Y, with complete data\nmod.Y = lm(Y ~ A + M + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 1: Controlling for predictor of Y\nmod.ate.1 = lm(Y ~ A + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 2: Treatment is randomized, no controls needed\nmod.ate.2 = lm(Y ~ A, data=dat.1)\n\n# All models put together\nmodels.full = list(\"M\" = mod.M, \"S\" = mod.S, \"Y\" = mod.Y, \"ATE (O-set)\" = mod.ate.1, \"ATE (no adj)\" = mod.ate.2)\n\n# Summary of models\nmodelsummary(models.full, statistic = \"[{conf.low}, {conf.high}]\",\n             estimate  = \"{estimate}{stars}\") \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> M </th>\n   <th style=\"text-align:center;\"> S </th>\n   <th style=\"text-align:center;\"> Y </th>\n   <th style=\"text-align:center;\"> ATE (O-set) </th>\n   <th style=\"text-align:center;\"> ATE (no adj) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> 0.104*** </td>\n   <td style=\"text-align:center;\"> −0.107+ </td>\n   <td style=\"text-align:center;\"> −1.579*** </td>\n   <td style=\"text-align:center;\"> −1.498*** </td>\n   <td style=\"text-align:center;\"> −1.524*** </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> [0.069, 0.139] </td>\n   <td style=\"text-align:center;\"> [−0.219, 0.005] </td>\n   <td style=\"text-align:center;\"> [−1.816, −1.342] </td>\n   <td style=\"text-align:center;\"> [−1.732, −1.263] </td>\n   <td style=\"text-align:center;\"> [−1.803, −1.245] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:center;\"> 0.506*** </td>\n   <td style=\"text-align:center;\"> 0.296*** </td>\n   <td style=\"text-align:center;\"> 0.281 </td>\n   <td style=\"text-align:center;\"> 0.677*** </td>\n   <td style=\"text-align:center;\"> 0.742*** </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> [0.456, 0.556] </td>\n   <td style=\"text-align:center;\"> [0.135, 0.457] </td>\n   <td style=\"text-align:center;\"> [−0.114, 0.675] </td>\n   <td style=\"text-align:center;\"> [0.340, 1.013] </td>\n   <td style=\"text-align:center;\"> [0.342, 1.141] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> L </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.427*** </td>\n   <td style=\"text-align:center;\"> 1.755*** </td>\n   <td style=\"text-align:center;\"> 1.768*** </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> [0.341, 0.514] </td>\n   <td style=\"text-align:center;\"> [1.584, 1.925] </td>\n   <td style=\"text-align:center;\"> [1.597, 1.940] </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.784*** </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;box-shadow: 0px 1.5px\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\"> [0.369, 1.200] </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Num.Obs. </td>\n   <td style=\"text-align:center;\"> 1000 </td>\n   <td style=\"text-align:center;\"> 1000 </td>\n   <td style=\"text-align:center;\"> 1000 </td>\n   <td style=\"text-align:center;\"> 1000 </td>\n   <td style=\"text-align:center;\"> 1000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 </td>\n   <td style=\"text-align:center;\"> 0.283 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.311 </td>\n   <td style=\"text-align:center;\"> 0.301 </td>\n   <td style=\"text-align:center;\"> 0.013 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 Adj. </td>\n   <td style=\"text-align:center;\"> 0.282 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.308 </td>\n   <td style=\"text-align:center;\"> 0.300 </td>\n   <td style=\"text-align:center;\"> 0.012 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> AIC </td>\n   <td style=\"text-align:center;\"> 1022.7 </td>\n   <td style=\"text-align:center;\"> 1278.2 </td>\n   <td style=\"text-align:center;\"> 4824.1 </td>\n   <td style=\"text-align:center;\"> 4835.8 </td>\n   <td style=\"text-align:center;\"> 5178.8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> BIC </td>\n   <td style=\"text-align:center;\"> 1037.5 </td>\n   <td style=\"text-align:center;\"> 1292.9 </td>\n   <td style=\"text-align:center;\"> 4848.7 </td>\n   <td style=\"text-align:center;\"> 4855.5 </td>\n   <td style=\"text-align:center;\"> 5193.5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Log.Lik. </td>\n   <td style=\"text-align:center;\"> −508.364 </td>\n   <td style=\"text-align:center;\"> −636.108 </td>\n   <td style=\"text-align:center;\"> −2407.072 </td>\n   <td style=\"text-align:center;\"> −2413.913 </td>\n   <td style=\"text-align:center;\"> −2586.403 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:center;\"> 394.163 </td>\n   <td style=\"text-align:center;\"> 51.919 </td>\n   <td style=\"text-align:center;\"> 149.554 </td>\n   <td style=\"text-align:center;\"> 214.731 </td>\n   <td style=\"text-align:center;\"> 13.281 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RMSE </td>\n   <td style=\"text-align:center;\"> 0.40 </td>\n   <td style=\"text-align:center;\"> 0.47 </td>\n   <td style=\"text-align:center;\"> 2.69 </td>\n   <td style=\"text-align:center;\"> 2.70 </td>\n   <td style=\"text-align:center;\"> 3.21 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nSince the treatment is randomized, no *control* variables are required in the regression of $Y$ against $A$ to remove confounding bias. However, $L$ is in the $O$-set of the effect; this is, controlling for $L$ produces an asymptotically more efficient estimator. Such property is not seen in finite samples \\[see last two models\\].\n\nIt is no surprise that, with complete data, the point-estimate for the coefficient of $A$ in the last two models lie close to the true ATE (0.75), even under a moderately noisy DGP ($R^2\\approx 0.3$).\n\n------------------------------------------------------------------------\n\n## Results under exclusion\n\n\n::: {.cell}\n\n:::\n\n\nIgnoring samples for which $S=0$ reduces sample size from 1000 to 514. This is, the probability of exclusion is about 0.486.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of observation per selection\n# 1 = selected / observed\n# 0 = excluded\ntable(dat.1$S) %>% kbl(col.names = c('S','N')) %>% kable_styling(full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> S </th>\n   <th style=\"text-align:right;\"> N </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 0 </td>\n   <td style=\"text-align:right;\"> 486 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 514 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nIn contrast with the complete data case, a regression-based approach for estimating the ATE with only the selected samples **does** require controlling for $L$, since conditioning on $S$ opens the non-causal path:\n\n-   $A\\longrightarrow S\\longleftarrow L\\longrightarrow Y$\n\nUnfortunately, controlling for $L$ alone **does not** remove selection bias, despite blocking such path. This is because **the distribution of** $L$ **in the sample does not necessarily match the distribution in the population**.\n\nLet us inspect it.\n\n### Regression-based approach:\n\nWe fit a model for the outcome $Y$ against $A$, controlling for $L$, using only the selected samples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data for the selected sample\ndat.s = dat.1 %>% filter(S==1)\nN.s = nrow(dat.s)\n\n# Model for causal effect, with selected\nmod.ate.s = lm(Y ~ A + L, data=dat.s)\nmodelsummary(list(\"ATE(S=1)\" = mod.ate.s), \n             statistic = NULL,\n             estimate  = \"{estimate}{stars} [{conf.low}, {conf.high}]\") \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> ATE(S=1) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> −1.341*** [−1.688, −0.995] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:center;\"> 0.516* [0.058, 0.974] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;box-shadow: 0px 1.5px\"> L </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\"> 1.843*** [1.601, 2.085] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Num.Obs. </td>\n   <td style=\"text-align:center;\"> 514 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 </td>\n   <td style=\"text-align:center;\"> 0.307 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 Adj. </td>\n   <td style=\"text-align:center;\"> 0.304 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> AIC </td>\n   <td style=\"text-align:center;\"> 2458.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> BIC </td>\n   <td style=\"text-align:center;\"> 2475.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Log.Lik. </td>\n   <td style=\"text-align:center;\"> −1225.026 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:center;\"> 113.215 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RMSE </td>\n   <td style=\"text-align:center;\"> 2.62 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe resulted 95% confidence interval for the coefficient of $A$, using only samples for which $S=1$, still covers the true ATE (0.75). Yet, a downwards bias shrinks the point-estimate and its lower bound noticeably. Using this, we would conclude the ATE is smaller than what truly is.\n\nLet us implement an IPW-based approach to compare against:\n\n### IPW-based approach:\n\nGiven the SCM, and the assumption of $\\gamma_2=0$, we can express:\n\n-   $w_i=\\mathbb{P}(S=1)/\\mathbb{P}(S=1\\mid A=a_i,L=l_i)$\n\n-   $v_i(a)=1/\\mathbb{P}(A=a) = 1/q$ because treatment is randomized\n\nThe derived finite-sample estimator of the mean counterfactuals / potential outcomes is: $$\n\\hat{Y}^a = N^{-1}\\sum_{i=1}^N\\frac{\\mathbb{I}(A_i=a)\\cdot\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\n$$\n\nPutting all ingredients together we get point-estimates $\\hat{Y}^1=\\hat{\\mathbb{E}}[Y\\mid do(A=1)]$ and $\\hat{Y}^0=\\hat{\\mathbb{E}}[Y\\mid do(A=0)]$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute the probability of selecton for all the units selected\n# We leverage the model mod.S, trained on complete data since S, A and L are\n# always observed (Y is the only one missing) and predict only on the selected units\nprob.s.unit = predict(mod.S, newdata=dat.s, type='response')\n\n# The unconditional probability of selection can be consistently estimated \n#... with counts from the total sample size and selected sample size\nprob.s = nrow(dat.s) / nrow(dat.1)\n\n# Since the treatment is randomized, the propensity score in the population is known at 0.5. \n# It can also be estimated from counts in the complete dataset\n# prop.score = sum(dat.1$A==1)/nrow(dat.1)\nprop.score = 0.5\n\n# Estimated counterfactuals/potential outcomes via IPPW\nest.PO = dat.s %>% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.Y = weights * Y) %>% group_by(A) %>% \n  summarise('PO'=sum(weighted.Y)/N.s)\n\n# Print resulted estimates\nhead(est.PO) %>% kbl() %>% kable_styling(full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> A </th>\n   <th style=\"text-align:right;\"> PO </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -1.4318076 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -0.7537995 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWhich, can be used to compute a point-estimate of the ATE: $\\hat{ATE}=\\hat{Y}^1-\\hat{Y}^0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print estimated ATE\nest.ATE = as.numeric(est.PO[2,2]-est.PO[1,2])\ndata.frame('IPW ATE'=est.ATE) %>% kbl() %>% kable_styling(full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> IPW.ATE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.678008 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWe can see that the IPW-based approach produces a point-estimate closer to the true ATE. To get valid confidence intervals, however, a bootstrapping or asymptotic analysis need to be invoked. We can compute naïve confidence interval without resampling, using the fact:\n\n$$\n\\hat{\\text{var}}(\\hat{\\text{ATE}}) \\geq \\hat{\\text{var}}(\\hat{Y}^1)+\\hat{\\text{var}}(\\hat{Y}^0)=(N-2)^{-2}\\sum_{a\\in\\{0,1\\}}\\sum_{i=1}^N \\mathbb{I}(A_i=a)\\cdot \\hat{v}_i(a)^2\\hat{w}_i^2(y_i-\\hat{Y}^a)^2\n$$Such variance is naïve in the sense that it is overconfident due to ignoring the covariance between the potential outcomes, and the higher-order contributions of the weighting factors in the total variance. Anyway, using it will get us:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimated counterfactuals/potential outcomes via IPPW\nsd.PO = dat.s %>% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.var = weights^2 * ( A* (Y-est.PO[2,2])^2 + (1-A)* (Y-est.PO[1,2])^2)) %>% \n  group_by(A) %>% \n  summarise('sd'=sqrt(sum(weighted.var))/(N.s-2))\n\n# Naïve confidence interval\nnaivebounds = qnorm(0.975)*sum(sd.PO[,2])\ndata.frame('IPW ATE'=as.numeric(est.ATE),\n           'naïve LB'=as.numeric(est.ATE)-naivebounds,\n           'naïve UB'=as.numeric(est.ATE)+naivebounds) %>% kbl() %>% kable_styling(full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> IPW.ATE </th>\n   <th style=\"text-align:right;\"> naïve.LB </th>\n   <th style=\"text-align:right;\"> naïve.UB </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.678008 </td>\n   <td style=\"text-align:right;\"> 0.5741569 </td>\n   <td style=\"text-align:right;\"> 0.7818592 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAs noted, the naïve confidence interval is tighter than those produced by inference on the complete data. This means such confidence interval is not statistically valid, but it can still help us visualize the convergence of IPW estimator.\n\nNow, if we simulate and repeat the DGP and same analysis for different sample sizes, consistency would be visually perceived if we see:\n\n-   Point-estimate converging to the true ATE\n\n-   Confidence intervals shrinking at a fast (\\*\\*) rate\n\nLet us test it. We run 20 simulations with different complete sample sizes, from $N=500$ to $N=23\\,000$ \\[number of complete samples from $N_s=250$ to $N_s=12\\,000$\\]. We repeat the procedure three times and average the results on those three repetitions. Such averages are presented in the following table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data frame to save results from iterations\nrounds.IPPW = data.frame(N=NA, Ns=NA, EST=NA, LB=NA, UB=NA)\n\n# All sample sizes to consider\nsamplesizes = round(500*exp(0.2*(0:19)))\n\n# Number of repetitions\nM = 3\n\n# Paramaters are the same as before\nparam.iter = param.1\n\n# Seed\nset.seed(66)\n\n# Loop\nfor(m in 1:M){\n  for(n in samplesizes){\n    \n    # Change sample size\n    param.iter[1] = n\n    \n    # Generate the data (complete and selected)\n    dat.iter = dgp(param.iter)[[1]]\n    dat.s.iter = dat.iter %>% filter(S==1)\n    \n    # Estimated probability of selection\n    N.s.iter = nrow(dat.s.iter)\n    prob.s.iter = N.s.iter / n\n    \n    # Model for S, with complete data\n    mod.S.iter = glm(S ~ A + L, family=binomial('probit'), data=dat.iter)\n    prob.s.unit.iter = predict(mod.S.iter, newdata=dat.s.iter, type='response')\n  \n    # Propensity score model\n    prop.score.iter = sum(dat.iter$A==1)/nrow(dat.iter)\n    \n    # Put everything together\n    row.iter = dat.s.iter %>% mutate(\n      prob.s.unit = as.numeric(prob.s.unit.iter),\n      prob.s = as.numeric(prob.s.iter),\n      prop.score = as.numeric(prop.score.iter),\n      pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n      weights = pro.weight * prob.s * (1/prob.s.unit),\n      weighted.Y = weights * Y) \n    \n    # Estimated counterfactuals/potential outcomes via IPPW\n    po.iter = row.iter %>% group_by(A) %>% \n      summarise('Y(A)'=sum(weighted.Y)/N.s.iter)\n    \n    # Estimated ATE\n    ATE.iter = as.numeric(po.iter[2,2]-po.iter[1,2])\n    \n    # Naïve variances\n    sd.PO.iter = row.iter %>% mutate(\n      weighted.var = weights^2 * ( A* (Y-po.iter[2,2])^2 + (1-A)* (Y-po.iter[1,2])^2)) %>% \n    group_by(A) %>% \n    summarise('sd'=sqrt(sum(weighted.var))/(N.s.iter-1))\n    \n    # Naïve confidence interval\n    naivebounds = qnorm(0.975)*sum(sd.PO.iter[,2])\n    \n    rounds.IPPW = rbind.data.frame(rounds.IPPW,\n                                   data.frame(N=n,\n                                              Ns=N.s.iter,\n                                              EST=ATE.iter,\n                                              LB=ATE.iter-naivebounds,\n                                              UB=ATE.iter+naivebounds))\n  }\n}\n# Print resulted estimates\nrounds.IPPW = rounds.IPPW[-1,] %>%\n  group_by(N) %>%\n  summarise_all(mean)\n\nhead(rounds.IPPW) %>% kbl() %>% kable_styling(full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> N </th>\n   <th style=\"text-align:right;\"> Ns </th>\n   <th style=\"text-align:right;\"> EST </th>\n   <th style=\"text-align:right;\"> LB </th>\n   <th style=\"text-align:right;\"> UB </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 500 </td>\n   <td style=\"text-align:right;\"> 249.6667 </td>\n   <td style=\"text-align:right;\"> 0.5919352 </td>\n   <td style=\"text-align:right;\"> -0.1461034 </td>\n   <td style=\"text-align:right;\"> 1.3299737 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 611 </td>\n   <td style=\"text-align:right;\"> 309.0000 </td>\n   <td style=\"text-align:right;\"> 0.7701710 </td>\n   <td style=\"text-align:right;\"> 0.1782066 </td>\n   <td style=\"text-align:right;\"> 1.3621354 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 746 </td>\n   <td style=\"text-align:right;\"> 379.6667 </td>\n   <td style=\"text-align:right;\"> 0.8962619 </td>\n   <td style=\"text-align:right;\"> 0.2991885 </td>\n   <td style=\"text-align:right;\"> 1.4933353 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 911 </td>\n   <td style=\"text-align:right;\"> 453.6667 </td>\n   <td style=\"text-align:right;\"> 0.7438565 </td>\n   <td style=\"text-align:right;\"> 0.5811669 </td>\n   <td style=\"text-align:right;\"> 0.9065460 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1113 </td>\n   <td style=\"text-align:right;\"> 556.0000 </td>\n   <td style=\"text-align:right;\"> 0.7817429 </td>\n   <td style=\"text-align:right;\"> 0.3579693 </td>\n   <td style=\"text-align:right;\"> 1.2055165 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1359 </td>\n   <td style=\"text-align:right;\"> 679.0000 </td>\n   <td style=\"text-align:right;\"> 0.6411555 </td>\n   <td style=\"text-align:right;\"> 0.3223225 </td>\n   <td style=\"text-align:right;\"> 0.9599884 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAn the following plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Plot results from rounds\nrounds.IPPW[,-1] %>% melt(id.vars = 'Ns') %>% \n  mutate(type = ifelse(variable=='EST','EST','N. C.I.') ) %>% \n  ggplot(aes(x=Ns,y=value,group=variable)) + \n  geom_point(aes(shape=type,color=type)) + \n  geom_smooth(method = lm, formula = y ~ x + I(sqrt(x)), se = FALSE) + ## O(N^0.5) convergence\n  labs(y='Value of estimate / bound', x='number of complete samples') +\n  geom_hline(yintercept=T.ATE, col = 'red') +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=720}\n:::\n:::\n\n\nWe can appreciate that the estimator converges to the true ATE \\[in red\\], and its uncertainty reduce at a sustained rate; there is **convergence in probability**. In other words, the IPW-based estimator is consistent.\n\n#### Mathematical justification of consistency\n\nWe can prove consistency mathematically for this SCM. However, to avoid measure-theoretic conundrums, let us consider the case for which all variables are discrete. Results are generalizable for mixed discrete-continuous cases with positive distributions (no zero-measure events).\n\nLet us assume $Y$ has support on $\\{y_{(c)}\\}_{c=1}^C$, and $L$ has support on $\\{l_{(k)}\\}_{k=1}^K$, then:\n\n$$\n\\begin{aligned}\n\\hat{Y}^a &= N^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\\cdot \\mathbb{I}(A_i=a) \\\\\n&= \nN^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a\\mid L=l_i)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a,L=l_i)}\\cdot y_i\\cdot\\mathbb{I}(A_i=a,S_i=1)\\\\\n&=\n\\sum_{i=1}^N\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_i)\n}\\cdot \n\\frac{\ny_i\\cdot\\mathbb{I}(A_i=a,S_i=1)/N}{ \n\\hat{\\mathbb{P}}(A=a\\mid L=l_i,S=1)\n}\\\\\n&= \\sum_{i=1}^N\\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot \n\\frac{\ny_{i}\\cdot\\mathbb{I}(A_i=a,L_i=l_{(k)},S_i=1)/N}{ \n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot \n\\frac{\ny_{(c)}\\cdot\\sum_{i=1}^N\\mathbb{I}(Y_i=y_{(c)},A_i=a,L_i=l_{(k)},S_i=1)/N}{ \n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot \n\\frac{\ny_{(c)}\\cdot\\hat{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{ \n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\n\\end{aligned}\n$$\n\nAssuming the propensity scores and the probability of selection are both correctly specified, all finite-sample approximations $\\hat{\\mathbb{P}}$ converge in the limit to the true distributions $\\mathbb{P}$. Then:\n\n$$\n\\begin{aligned}\n\\text{plim}_{N\\rightarrow\\infty}\n\\hat{Y}^a\n &= \n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(S=1)}{\n{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot \n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{ \n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(L=l_{(k)})}{\n{\\mathbb{P}}(L=l_{(k)}\\mid S=1)\n}\\cdot \n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{ \n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \n\\sum_{k}\\sum_{c}\n{\\mathbb{P}}(L=l_{(k)})\n\\cdot \ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)} S=1)\\\\\n&= \n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\sum_{c}\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)}, S=1)\\\\\n&= \n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\mathbb{E}(Y\\mid A=a,L, S=1)\\\\\n&= \n\\mathbb{E}_L\n\\mathbb{E}(Y\\mid A=a,L, S=1) =\\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L,S=1]\\\\\n&= \\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L] = \\mathbb{E}[Y\\mid do(A=a)]\n\\end{aligned}\n$$\n\nThis is, the IPW estimator converges to the true counterfactual mean given by intervention $do(A=a)$. The last two equalities come from these facts:\n\n-   $L$ blocks all non-causal paths from $A$ to $Y$ when conditioning on $S=1$\n\n-   $Y\\perp S\\,\\mid L$ in the back-door graph: the resulting DAG after removing all arrows coming out of $A$\n\n-   $\\mathbb{P}(L=l_{(k)})$ is the population-distribution of $L$\n\n### Revisiting the regression approach: generalized adjustment criteria\n\nNotice that, in the convergence proof for the IPW estimator, it was shown that the underlying estimand is algebraically equivalent to a regression-adjusted mean of $Y$, averaged over the population distribution of $L$.\n\n$$\n\\mathbb{E}[Y\\mid do(A=a)] = \\mathbb{E}_L\\mathbb{E}(Y\\mid A=a,L,S=1)\n$$\n\nExtensions of this results are covered by three ***generalized adjustment criteria*** [@Correa_Tian_Bareinboim_2018]. This is, for this case, a mathematically equivalent result in term of consistency can be achieved via regression adjustment.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}